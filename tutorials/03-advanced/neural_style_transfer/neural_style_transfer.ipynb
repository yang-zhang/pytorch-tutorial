{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
    "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "    \n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    \n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image.to(device)\n",
    "\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28'] \n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    \n",
    "    # Image preprocessing\n",
    "    # VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
    "    # We use the same normalization statistics here.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load content and style images\n",
    "    # Make the style image same size as the content image\n",
    "    content = load_image(config.content, transform, max_size=config.max_size)\n",
    "    style = load_image(config.style, transform, shape=[content.size(2), content.size(3)])\n",
    "    \n",
    "    # Initialize a target image with the content image\n",
    "    target = content.clone().requires_grad_(True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[0.5, 0.999])\n",
    "    vgg = VGGNet().to(device).eval()\n",
    "    \n",
    "    for step in range(config.total_step):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        target_features = vgg(target)\n",
    "        content_features = vgg(content)\n",
    "        style_features = vgg(style)\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            # Compute content loss with target and content images\n",
    "            content_loss += torch.mean((f1 - f2)**2)\n",
    "\n",
    "            # Reshape convolutional feature maps\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "\n",
    "            # Compute gram matrix\n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "\n",
    "            # Compute style loss with target and style images\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "        \n",
    "        # Compute total loss, backprop and optimize\n",
    "        loss = content_loss + config.style_weight * style_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step+1) % config.log_step == 0:\n",
    "            print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n",
    "                   .format(step+1, config.total_step, content_loss.item(), style_loss.item()))\n",
    "\n",
    "        if (step+1) % config.sample_step == 0:\n",
    "            # Save the generated image\n",
    "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().squeeze()\n",
    "            img = denorm(img).clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--content', type=str, default='png/content.png')\n",
    "parser.add_argument('--style', type=str, default='png/style.png')\n",
    "parser.add_argument('--max_size', type=int, default=400)\n",
    "parser.add_argument('--total_step', type=int, default=2000)\n",
    "parser.add_argument('--log_step', type=int, default=10)\n",
    "parser.add_argument('--sample_step', type=int, default=500)\n",
    "parser.add_argument('--style_weight', type=float, default=100)\n",
    "parser.add_argument('--lr', type=float, default=0.003)\n",
    "\n",
    "\n",
    "# python main.py --content='png/content.png' --style='png/style.png'\n",
    "config = parser.parse_args(\"--content=png/content.png --style=png/style.png\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
